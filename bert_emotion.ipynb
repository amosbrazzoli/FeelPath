{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python387jvsc74a57bd0fce919f00e395b6eaae18fc4c94b361522f2206a1f6ad63b70b9383a62ddd33b",
   "display_name": "Python 3.8.7 64-bit ('FeelPath': pipenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "fce919f00e395b6eaae18fc4c94b361522f2206a1f6ad63b70b9383a62ddd33b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as o\n",
    "import torch.utils.data as d\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import transformers as T\n",
    "\n",
    "from datasets import DailyDialog\n",
    "from models import BERTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T.BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,model_max_length=MAX_LEN)\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_process(data):\n",
    "    uniques = set(data)\n",
    "    return data, {\"categories\" : uniques}\n",
    "\n",
    "def data_process(data):\n",
    "    out = []\n",
    "    lens = []\n",
    "    for i in data:\n",
    "        i = i.strip('\". ')\n",
    "        i = tokenizer.tokenize(i)\n",
    "        \n",
    "\n",
    "        lens.append(len(i))\n",
    "        i = tokenizer.convert_tokens_to_ids(i)\n",
    "        if len(i) > MAX_LEN:\n",
    "            i = i[:MAX_LEN]\n",
    "        i += [PAD_INDEX] * (MAX_LEN - len(i))\n",
    "        out.append(torch.tensor(i))\n",
    "    max_len = max(lens)\n",
    "    return out, {\"max_len\" : max_len, \"lengths\" : lens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DailyDialog()\n",
    "dataset.load()\n",
    "dataset.train_preprocess(data_process)\n",
    "dataset.test_preprocess(target_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = dataset.args[\"train\"][\"lengths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f631f1953a0>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 360x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "sns.displot(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier()\n",
    "criterion = o.Adam(model.parameters(), lr=2e-5)\n",
    "NUM_EPOCHS = 5\n",
    "eval_interval = len(dataset) // 2\n",
    "SAVE_PATH = 'save'\n",
    "best_valid = float(\"Inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1000, 50]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "dataloader = d.DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "for x_batch, y_batch in dataloader:\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.00\n",
    "valid_running_loss = 0.00\n",
    "global_step = 0\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "global_steps_list = []\n",
    "\n",
    "validate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/103 [00:00<?, ?it/s]Predicting\n",
      "Masked\n",
      "  0%|          | 0/103 [00:24<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b68ce3935305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initialising Optimizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/FeelPath/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Masked\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         )\n\u001b[0;32m--> 971\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    566\u001b[0m                 )\n\u001b[1;32m    567\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     ):\n\u001b[0;32m--> 387\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/FeelPath-wtWy25uN/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for x_batch, y_batch in tqdm(dataloader):\n",
    "        print(\"Predicting\")\n",
    "        loss = model(x_batch)\n",
    "\n",
    "        print(\"Initialising Optimizer\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(\"Computing Loss\")\n",
    "        loss.backward()\n",
    "\n",
    "        print(\"Optimising\")\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        global_step += 1\n",
    "\n",
    "        if (validate) and (global_step % eval_interval == 0):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for x_val_batch, y_val_batch in valid_dataloader:\n",
    "                    loss, _ = model(x_val_batch, y_val_batch)\n",
    "                    valid_running_loss += loss.item()\n",
    "\n",
    "            average_train_loss = running_loss / eval_interval\n",
    "            average_valid_loss = valid_running_loss / len(valid_dataloader)\n",
    "\n",
    "            train_loss_list.append(average_train_loss)\n",
    "            valid_loss_list.append(average_valid_loss)\n",
    "\n",
    "            global_step_list.append(global_step)\n",
    "\n",
    "            # resetting loss values\n",
    "            running_loss = 0.00\n",
    "            valid_running_loss = 0.00\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            # print progress\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                    .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                            average_train_loss, average_valid_loss))\n",
    "\n",
    "            # checkpoint\n",
    "            if best_valid_loss > average_valid_loss:\n",
    "                best_valid_loss = average_valid_loss\n",
    "                save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "                save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "\n",
    "        save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "        print('Finished Training!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}